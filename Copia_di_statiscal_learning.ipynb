{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristianMorasso/statistical_project/blob/master/Copia_di_statiscal_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Learning Project: ALFA Dataset Anaysis\n"
      ],
      "metadata": {
        "id": "DFS000cMNXMb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYUOovxy7XZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3bbae6-8882-42c9-900c-043c954f0f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: icecream in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: sktime in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.20.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from icecream) (0.4.6)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.16.1)\n",
            "Requirement already satisfied: executing>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sktime) (24.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=1.1 in /usr/local/lib/python3.10/dist-packages (from sktime) (2.0.3)\n",
            "Requirement already satisfied: scikit-base<0.8.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sktime) (0.7.8)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2024.1)\n",
            "fatal: destination path 'statistical_project' already exists and is not an empty directory.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install icecream sktime scikit-learn polars numpy\n",
        "!rm -rf statiscal_project\n",
        "!git clone https://github.com/CristianMorasso/statistical_project.git\n",
        "!pwd\n",
        "# # Connect Drive to get the Dataset\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive',force_remount=True)\n",
        "# dataset_dir= '/content/drive/MyDrive/alfa_dataset'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"statistical_project/data\""
      ],
      "metadata": {
        "id": "H-QpqZI1Pw_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from the csv files\n",
        "import numpy as np\n",
        "import pandas  as pd\n",
        "import os\n",
        "\n",
        "dfs = [] # dataframe list\n",
        "class_dict = {}\n",
        "\n",
        "class_id = 0\n",
        "y_time_series = []\n",
        "\n",
        "for cl  in  os.listdir(dataset_dir):\n",
        "  path1 = os.path.join(dataset_dir, cl)\n",
        "  if not os.path.isdir(path1) or cl == \"features\":\n",
        "    print(f\"{cl} is not a dir or not useful!\")\n",
        "    continue\n",
        "  class_dict[class_id] = cl\n",
        "  print(f\"Class: {cl}\")\n",
        "  for f in  os.listdir(path1):\n",
        "    print(f\"\\t File:{f}\")\n",
        "    pathf = os.path.join(path1, f)\n",
        "    df = pd.read_csv(pathf)\n",
        "\n",
        "    # Remove Label field\n",
        "    label_field = \"field.data\"\n",
        "    try:\n",
        "      df = df.drop(label_field, axis=1)\n",
        "    except:\n",
        "      print(f\"\\t- The {label_field}~ is not in the {f} file\")\n",
        "\n",
        "    # Save the id for the serie\n",
        "    y_time_series.append(class_id)\n",
        "\n",
        "    # This would be useless but the fit algorithm needs this\n",
        "    df.insert(0, \"Y\", [ class_id for _ in range(len(df))], True)\n",
        "    dfs.append(df)\n",
        "\n",
        "  class_id+=1\n",
        "  print() # Skip line\n",
        "\n",
        "y_time_series = np.array(y_time_series)\n",
        "\n",
        "# Checks\n",
        "from collections import Counter\n",
        "print(f\"Y: {Counter(list(map(lambda x: class_dict[x], y_time_series)))}\")\n",
        "\n",
        "print(f\"\\nNumber of Time Series: {len(dfs)}\")\n",
        "assert len(dfs) == 46  # We know this from the repo"
      ],
      "metadata": {
        "id": "nCaKGDiag_ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410fe85d-c35b-486c-8d0f-f96fdd4e4b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: ailerons\n",
            "\t File:carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure.csv\n",
            "\t File:carbonZ_2018-10-05-14-37-22_2_right_aileron_failure.csv\n",
            "\t File:carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure.csv\n",
            "\t File:carbonZ_2018-09-11-17-55-30_2_left_aileron_failure.csv\n",
            "\t File:carbonZ_2018-10-05-14-37-22_3_left_aileron_failure.csv\n",
            "\t File:carbonZ_2018-09-11-17-55-30_1_right_aileron_failure.csv\n",
            "\n",
            "Class: elevator\n",
            "\t File:carbonZ_2018-09-11-15-05-11_1_elevator_failure.csv\n",
            "\t File:carbonZ_2018-09-11-14-41-51_elevator_failure.csv\n",
            "\n",
            "Class: rudder\n",
            "\t File:carbonZ_2018-09-11-15-06-34_2_rudder_right_failure.csv\n",
            "\t File:carbonZ_2018-09-11-15-06-34_3_rudder_left_failure.csv\n",
            "\t File:carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure.csv\n",
            "\t File:carbonZ_2018-09-11-15-06-34_1_rudder_right_failure.csv\n",
            "\n",
            "README.md is not a dir or not useful!\n",
            "Class: no_failure\n",
            "\t File:carbonZ_2018-09-11-14-16-55_no_failure.csv\n",
            "\t- The field.data~ is not in the carbonZ_2018-09-11-14-16-55_no_failure.csv file\n",
            "\t File:carbonZ_2018-10-05-14-34-20_1_no_failure.csv\n",
            "\t- The field.data~ is not in the carbonZ_2018-10-05-14-34-20_1_no_failure.csv file\n",
            "\t File:carbonZ_2018-09-11-14-41-38_no_failure.csv\n",
            "\t- The field.data~ is not in the carbonZ_2018-09-11-14-41-38_no_failure.csv file\n",
            "\t File:carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-10-05-15-52-12_1_no_failure.csv\n",
            "\t- The field.data~ is not in the carbonZ_2018-10-05-15-52-12_1_no_failure.csv file\n",
            "\t File:carbonZ_2018-07-18-16-37-39_1_no_failure.csv\n",
            "\t- The field.data~ is not in the carbonZ_2018-07-18-16-37-39_1_no_failure.csv file\n",
            "\t File:carbonZ_2018-09-11-15-05-11_2_no_failure.csv\n",
            "\t- The field.data~ is not in the carbonZ_2018-09-11-15-05-11_2_no_failure.csv file\n",
            "\t File:carbonZ_2018-10-05-15-52-12_2_no_failure.csv\n",
            "\t- The field.data~ is not in the carbonZ_2018-10-05-15-52-12_2_no_failure.csv file\n",
            "\t File:carbonZ_2018-07-30-16-39-00_3_no_failure.csv\n",
            "\t- The field.data~ is not in the carbonZ_2018-07-30-16-39-00_3_no_failure.csv file\n",
            "\t File:carbonZ_2018-10-05-14-37-22_1_no_failure.csv\n",
            "\t- The field.data~ is not in the carbonZ_2018-10-05-14-37-22_1_no_failure.csv file\n",
            "\n",
            "__init__.py is not a dir or not useful!\n",
            "features is not a dir or not useful!\n",
            "Class: engine\n",
            "\t File:carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-07-30-16-39-00_2_engine_failure.csv\n",
            "\t File:carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-07-30-16-39-00_1_engine_failure.csv\n",
            "\t File:carbonZ_2018-09-11-11-56-30_engine_failure.csv\n",
            "\t File:carbonZ_2018-09-11-14-22-07_1_engine_failure.csv\n",
            "\t File:carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-09-11-14-22-07_2_engine_failure.csv\n",
            "\t File:carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-07-18-15-53-31_1_engine_failure.csv\n",
            "\t File:carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj.csv\n",
            "\t File:carbonZ_2018-07-18-15-53-31_2_engine_failure.csv\n",
            "\n",
            "Y: Counter({'engine': 23, 'no_failure': 10, 'ailerons': 7, 'rudder': 4, 'elevator': 2})\n",
            "\n",
            "Number of Time Series: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before: \")\n",
        "for df in dfs:\n",
        "  print(df.shape)\n",
        "\n",
        "# Only use the variables that can be found in all the dataframes\n",
        "features = []\n",
        "# Get the biggest set:\n",
        "# (We are considering that that the bigger set is just a superset and has all the other base features)\n",
        "for df in dfs:\n",
        "   features = max(features, df.columns.values, key=lambda x: len(x))\n",
        "# Then get the set intersection of all of the features sets\n",
        "features = set(features)\n",
        "for df in dfs:\n",
        "  features = features.intersection(set(df.columns.values))\n",
        "\n",
        "# Apply the feature mask to all the datframe in our dataset\n",
        "features = list(features)\n",
        "dfs = list(map(lambda x: x[features],dfs))\n",
        "\n",
        "print(\"\\nAfter: \")\n",
        "for df in dfs:\n",
        "  print(df.shape)"
      ],
      "metadata": {
        "id": "PdPkM7MQP36Q",
        "outputId": "df156902-456e-4a52-bda3-d718c1ddab35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: \n",
            "(3242, 27)\n",
            "(4668, 27)\n",
            "(2899, 27)\n",
            "(2032, 27)\n",
            "(1628, 27)\n",
            "(1928, 27)\n",
            "(2662, 27)\n",
            "(1521, 27)\n",
            "(2568, 27)\n",
            "(1383, 27)\n",
            "(1387, 27)\n",
            "(2869, 27)\n",
            "(1405, 27)\n",
            "(670, 27)\n",
            "(1335, 27)\n",
            "(865, 27)\n",
            "(2330, 27)\n",
            "(1792, 27)\n",
            "(607, 27)\n",
            "(1347, 27)\n",
            "(969, 27)\n",
            "(1581, 27)\n",
            "(1454, 27)\n",
            "(2660, 27)\n",
            "(2844, 27)\n",
            "(2252, 27)\n",
            "(2185, 27)\n",
            "(2124, 27)\n",
            "(2263, 27)\n",
            "(2632, 27)\n",
            "(2488, 27)\n",
            "(2282, 27)\n",
            "(2330, 27)\n",
            "(2450, 27)\n",
            "(2292, 27)\n",
            "(1331, 27)\n",
            "(1247, 27)\n",
            "(3137, 27)\n",
            "(2609, 27)\n",
            "(2649, 27)\n",
            "(2328, 27)\n",
            "(2134, 27)\n",
            "(2356, 27)\n",
            "(2646, 27)\n",
            "(1844, 27)\n",
            "(1775, 27)\n",
            "\n",
            "After: \n",
            "(3242, 27)\n",
            "(4668, 27)\n",
            "(2899, 27)\n",
            "(2032, 27)\n",
            "(1628, 27)\n",
            "(1928, 27)\n",
            "(2662, 27)\n",
            "(1521, 27)\n",
            "(2568, 27)\n",
            "(1383, 27)\n",
            "(1387, 27)\n",
            "(2869, 27)\n",
            "(1405, 27)\n",
            "(670, 27)\n",
            "(1335, 27)\n",
            "(865, 27)\n",
            "(2330, 27)\n",
            "(1792, 27)\n",
            "(607, 27)\n",
            "(1347, 27)\n",
            "(969, 27)\n",
            "(1581, 27)\n",
            "(1454, 27)\n",
            "(2660, 27)\n",
            "(2844, 27)\n",
            "(2252, 27)\n",
            "(2185, 27)\n",
            "(2124, 27)\n",
            "(2263, 27)\n",
            "(2632, 27)\n",
            "(2488, 27)\n",
            "(2282, 27)\n",
            "(2330, 27)\n",
            "(2450, 27)\n",
            "(2292, 27)\n",
            "(1331, 27)\n",
            "(1247, 27)\n",
            "(3137, 27)\n",
            "(2609, 27)\n",
            "(2649, 27)\n",
            "(2328, 27)\n",
            "(2134, 27)\n",
            "(2356, 27)\n",
            "(2646, 27)\n",
            "(1844, 27)\n",
            "(1775, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection"
      ],
      "metadata": {
        "id": "o5Vh1IDyXE9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "# Concat all the time series in as single dataframe\n",
        "concatted_dfs = pd.concat(dfs)\n",
        "y = concatted_dfs[\"Y\"]\n",
        "\n",
        "# Now remove it again from the concatted dataframe\n",
        "concatted_dfs = concatted_dfs.drop(\"Y\", axis=1)\n",
        "# Also remove the time column!\n",
        "concatted_dfs = concatted_dfs.drop(\"%time\", axis=1)\n",
        "\n",
        "# Print the number of nans per per each columns\n",
        "print(concatted_dfs.isnull().sum())\n",
        "print(f\"Length of Concatted Dataframes: {len(concatted_dfs)}\")"
      ],
      "metadata": {
        "id": "gTeZ6pSyXGWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6bd5cb-8bca-4a17-8319-479c62c5f39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta_yaw                       57\n",
            "field.aspd_error                57\n",
            "field.angular_velocity.y       294\n",
            "field.twist.linear.z           145\n",
            "field.alt_error                 57\n",
            "delta_z                         59\n",
            "delta_roll                      59\n",
            "delta_roll_airspeed             60\n",
            "field.orientation.z            294\n",
            "field.angular_velocity.x       294\n",
            "delta_y                         59\n",
            "field.linear_acceleration.y    294\n",
            "field.linear_acceleration.z    294\n",
            "field.twist.linear.y           145\n",
            "field.twist.angular.z          145\n",
            "delta_pitch                     57\n",
            "field.angular_velocity.z       294\n",
            "delta_x                         59\n",
            "field.xtrack_error              57\n",
            "field.twist.angular.x          145\n",
            "field.orientation.x            294\n",
            "field.orientation.y            294\n",
            "field.twist.linear.x           145\n",
            "field.twist.angular.y          145\n",
            "field.linear_acceleration.x    294\n",
            "dtype: int64\n",
            "Length of Concatted Dataframes: 96000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inpute the nan values to the mean of the other entries value of the same column\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp.fit(concatted_dfs)\n",
        "inp_out = imp.transform(concatted_dfs)"
      ],
      "metadata": {
        "id": "X7tiW05ZEoPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best 20 features of the dataset\n",
        "columns_ = concatted_dfs.columns.values\n",
        "feature_mask= SelectKBest(k=20).fit(inp_out, y).get_feature_names_out(columns_)\n",
        "# Uses: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif\n",
        "\n",
        "print(\"Feature Mask:\")\n",
        "print(feature_mask)\n",
        "print(f\"Length of Feature Mask: {len(feature_mask)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_23zZl7LCXTj",
        "outputId": "26d77ea8-b3cb-4c16-8d06-2cc80fb1762e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Mask:\n",
            "['delta_yaw' 'field.aspd_error' 'field.angular_velocity.y'\n",
            " 'field.twist.linear.z' 'field.alt_error' 'delta_z' 'delta_roll_airspeed'\n",
            " 'delta_y' 'field.linear_acceleration.y' 'field.linear_acceleration.z'\n",
            " 'field.twist.linear.y' 'field.twist.angular.z' 'delta_pitch'\n",
            " 'field.angular_velocity.z' 'delta_x' 'field.xtrack_error'\n",
            " 'field.orientation.x' 'field.orientation.y' 'field.twist.angular.y'\n",
            " 'field.linear_acceleration.x']\n",
            "Length of Feature Mask: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now finally use the mask on every dataframe in the original dataframeList\n",
        "# and also inpute but using time-series specific data:\n",
        "new_dfs = list(map(lambda x: SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x[feature_mask]) , dfs))\n",
        "\n",
        "# Now we have the 20 best features without any nan value:\n",
        "type(new_dfs[0]) # is a numpy.ndarray! wow!\n",
        "print(new_dfs[0][:2,:]) # First 2 lines\n",
        "print(new_dfs[0].shape) # shape of the array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4SWHHfMYFWn",
        "outputId": "10408542-3474-45f6-d0b4-87c0083d24de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.89663155e+02 -3.45677948e+02 -1.01050220e-01 -9.80125368e-01\n",
            "   1.79999992e-01 -9.24748778e-01  1.50000000e+01  1.80730991e+01\n",
            "   6.12022871e-01  1.10048680e+01  2.43065834e+00 -1.40247777e-01\n",
            "   3.60856056e+00 -6.21675979e-02  1.32870197e+01 -4.14283066e+01\n",
            "  -3.07238990e-02 -5.50597940e-02 -4.29745093e-02  2.03362941e-01]\n",
            " [ 1.89663155e+02 -3.45677948e+02 -1.01050220e-01 -9.80125368e-01\n",
            "   1.79999992e-01 -8.96695733e-01  1.60000000e+01  1.80944805e+01\n",
            "   6.12022871e-01  1.10048680e+01  2.43065834e+00 -1.40247777e-01\n",
            "   3.61856055e+00 -6.21675979e-02  1.31595628e+01 -4.14283066e+01\n",
            "  -3.07238990e-02 -5.50597940e-02 -4.29745093e-02  2.03362941e-01]]\n",
            "(3242, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now finally use the mask on every dataframe in the original\n",
        "# dataframeList and also inpute but using time-series specific data:\n",
        "new_dfs = list(map(lambda x: SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x[feature_mask]) , dfs))\n",
        "\n",
        "# Now we have the 20 best features without any nan value:\n",
        "type(new_dfs[0]) # is a numpy.ndarray! wow!\n",
        "print(new_dfs[0][:2,:]) # First 2 lines\n",
        "print(new_dfs[0].shape) # shape of the array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8V6bQI5Zwst",
        "outputId": "f255a855-7d64-4c61-b696-6cd6981b8288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.89663155e+02 -3.45677948e+02 -1.01050220e-01 -9.80125368e-01\n",
            "   1.79999992e-01 -9.24748778e-01  1.50000000e+01  1.80730991e+01\n",
            "   6.12022871e-01  1.10048680e+01  2.43065834e+00 -1.40247777e-01\n",
            "   3.60856056e+00 -6.21675979e-02  1.32870197e+01 -4.14283066e+01\n",
            "  -3.07238990e-02 -5.50597940e-02 -4.29745093e-02  2.03362941e-01]\n",
            " [ 1.89663155e+02 -3.45677948e+02 -1.01050220e-01 -9.80125368e-01\n",
            "   1.79999992e-01 -8.96695733e-01  1.60000000e+01  1.80944805e+01\n",
            "   6.12022871e-01  1.10048680e+01  2.43065834e+00 -1.40247777e-01\n",
            "   3.61856055e+00 -6.21675979e-02  1.31595628e+01 -4.14283066e+01\n",
            "  -3.07238990e-02 -5.50597940e-02 -4.29745093e-02  2.03362941e-01]]\n",
            "(3242, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally let's create a multiIndexed pandas.Dataframe so that we can use our data with the sktime library\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jc_ssBlWcj0o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}